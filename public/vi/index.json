[
{
	"uri": "http://localhost:1313/vi/2-preparation/2.1-iam-role/",
	"title": "Tạo IAM role",
	"tags": [],
	"description": "",
	"content": "Tạo IAM Role Trong bước này, chúng ta sẽ chuyển tới giao diện IAM Console và tạo role cho dịch vụ Glue. Role này sẽ cho phép AWS Glue truy cập tới dữ liệu nằm trong S3 và tạo các đối tượng cần thiết trong Glue Catalog.\nTruy cập vào giao diện AWS Management Console\nTìm IAM Chọn IAM để vào IAM Dashboard Trong IAM Dashboard\nChọn Role Chọn Create role Trong giao diện Create role, ở bước Select trusted entity\nChọn AWS Service Ở phần Service or use case chọn Glue Chọn Next Trong giao diện Create role, ở bước Add Permission\nTìm và chọn AmazonS3FullAccess Tìm và chọn AWSGlueServiceRole Chọn Next Trong giao diện Create role, ở bước Name, review and create\nỞ Role name, đặt tên AWSGlueServiceRoleDefault Xem lại thông tin role tại Select trusted entity và Add Permission Chọn Create role Giao diện tạo role thành công: "
},
{
	"uri": "http://localhost:1313/vi/",
	"title": "Data Lake",
	"tags": [],
	"description": "",
	"content": "Tìm hiểu về Data Lake trên AWS Giới thiệu Data Lake Data Lake là một hệ thống lưu trữ tập trung cho phép lưu trữ dữ liệu dưới mọi định dạng - có cấu trúc, bán cấu trúc hoặc không cấu trúc. Không giống như các cơ sở dữ liệu truyền thống yêu cầu định dạng dữ liệu trước khi lưu trữ, Data Lake cho phép lưu trữ dữ liệu thô và xử lý hoặc phân tích khi cần.\nLợi ích của Data Lake Lưu trữ linh hoạt: Hỗ trợ dữ liệu từ nhiều nguồn và dưới nhiều định dạng khác nhau.\nPhân tích toàn diện: Tạo điều kiện cho phân tích dữ liệu lớn và các ứng dụng AI/ML.\nTiết kiệm chi phí: Sử dụng các giải pháp lưu trữ chi phí thấp như Amazon S3.\nTích hợp dễ dàng: Kết nối mượt mà với các công cụ phân tích và báo cáo như Amazon Athena và QuickSight.\nThách thức khi triển khai Data Lake Quản Lý Dữ Liệu: Làm thế nào để tổ chức và quản lý dữ liệu hiệu quả?\nBảo Mật: Làm sao để ngăn chặn truy cập trái phép vào dữ liệu?\nKhả Năng Mở Rộng: Hệ thống phải có khả năng mở rộng để xử lý sự gia tăng dữ liệu.\nHiệu Suất: Cần tối ưu hóa cho việc truy vấn và xử lý dữ liệu.\nChất Lượng Dữ Liệu: Đảm bảo tính chính xác và độ tin cậy của dữ liệu là rất quan trọng.\nKiến trúc Workshop Tổng quan về kiến trúc Hình dưới đây minh họa kiến trúc hệ thống Data Lake mà chúng ta sẽ triển khai trong workshop này: Mô tả kiến trúc Thu thập dữ liệu:\nDữ liệu từ nhiều nguồn khác nhau được thu thập qua Kinesis. Kinesis Firehose Stream xử lý và chuyển dữ liệu đến Amazon S3. Lưu trữ dữ liệu thô:\nDữ liệu thô được lưu trữ trong S3 ở thư mục \u0026ldquo;raw data\u0026rdquo;. CloudFormation tự động triển khai các tài nguyên cần thiết. AWS Glue:\nAWS Glue Crawler quét dữ liệu thô trong S3 để tạo metadata. Metadata được lưu trữ trong AWS Glue Data Catalog. Một ETL Job (Extract, Transform, Load) xử lý và chuyển đổi dữ liệu thô thành dữ liệu đã qua xử lý. Lưu trữ dữ liệu đã xử lý:\nDữ liệu đã được chuyển đổi được lưu trong một bucket S3 khác dưới thư mục \u0026ldquo;processed-data\u0026rdquo;. Phân tích và trực quan hóa dữ liệu:\nAWS Glue Crawler quét dữ liệu đã qua xử lý và cập nhật Glue Data Catalog. Amazon Athena được sử dụng để truy vấn dữ liệu trong S3. Amazon QuickSight kết nối đến dữ liệu để trực quan hóa và báo cáo. Mục tiêu của workshop Hiểu các thành phần của kiến trúc Data Lake. Triển khai một hệ thống Data Lake đơn giản bằng các dịch vụ AWS. Tích hợp các công cụ phân tích và trực quan hóa để trích xuất thông tin hữu ích từ dữ liệu. Nội dung: Giới thiệu Các bước chuẩn bị Thu thập và lưu trữ dữ liệu Tạo Data Catalog Chuyển đổi dữ liệu Phân tích và trực quan hóa Data Warehouse Dọn dẹp tài nguyên "
},
{
	"uri": "http://localhost:1313/vi/1-introduce/",
	"title": "Giới thiệu",
	"tags": [],
	"description": "",
	"content": "Tìm hiểu về Data Lake trên AWS Giới thiệu Data Lake Data Lake là một hệ thống lưu trữ tập trung cho phép lưu trữ dữ liệu dưới mọi định dạng - có cấu trúc, bán cấu trúc hoặc không cấu trúc. Không giống như các cơ sở dữ liệu truyền thống yêu cầu định dạng dữ liệu trước khi lưu trữ, Data Lake cho phép lưu trữ dữ liệu thô và xử lý hoặc phân tích khi cần.\nLợi ích của Data Lake Lưu trữ linh hoạt: Hỗ trợ dữ liệu từ nhiều nguồn và dưới nhiều định dạng khác nhau.\nPhân tích toàn diện: Tạo điều kiện cho phân tích dữ liệu lớn và các ứng dụng AI/ML.\nTiết kiệm chi phí: Sử dụng các giải pháp lưu trữ chi phí thấp như Amazon S3.\nTích hợp dễ dàng: Kết nối mượt mà với các công cụ phân tích và báo cáo như Amazon Athena và QuickSight.\nThách thức khi triển khai Data Lake Quản Lý Dữ Liệu: Làm thế nào để tổ chức và quản lý dữ liệu hiệu quả?\nBảo Mật: Làm sao để ngăn chặn truy cập trái phép vào dữ liệu?\nKhả Năng Mở Rộng: Hệ thống phải có khả năng mở rộng để xử lý sự gia tăng dữ liệu.\nHiệu Suất: Cần tối ưu hóa cho việc truy vấn và xử lý dữ liệu.\nChất Lượng Dữ Liệu: Đảm bảo tính chính xác và độ tin cậy của dữ liệu là rất quan trọng.\nKiến trúc Workshop Tổng quan về kiến trúc Hình dưới đây minh họa kiến trúc hệ thống Data Lake mà chúng ta sẽ triển khai trong workshop này: Mô tả kiến trúc Thu thập dữ liệu:\nDữ liệu từ nhiều nguồn khác nhau được thu thập qua Kinesis. Kinesis Firehose Stream xử lý và chuyển dữ liệu đến Amazon S3. Lưu trữ dữ liệu thô:\nDữ liệu thô được lưu trữ trong S3 ở thư mục \u0026ldquo;raw data\u0026rdquo;. CloudFormation tự động triển khai các tài nguyên cần thiết. AWS Glue:\nAWS Glue Crawler quét dữ liệu thô trong S3 để tạo metadata. Metadata được lưu trữ trong AWS Glue Data Catalog. Một ETL Job (Extract, Transform, Load) xử lý và chuyển đổi dữ liệu thô thành dữ liệu đã qua xử lý. Lưu trữ dữ liệu đã xử lý:\nDữ liệu đã được chuyển đổi được lưu trong một bucket S3 khác dưới thư mục \u0026ldquo;processed-data\u0026rdquo;. Phân tích và trực quan hóa dữ liệu:\nAWS Glue Crawler quét dữ liệu đã qua xử lý và cập nhật Glue Data Catalog. Amazon Athena được sử dụng để truy vấn dữ liệu trong S3. Amazon QuickSight kết nối đến dữ liệu để trực quan hóa và báo cáo. Mục tiêu của workshop Hiểu các thành phần của kiến trúc Data Lake. Triển khai một hệ thống Data Lake đơn giản bằng các dịch vụ AWS. Tích hợp các công cụ phân tích và trực quan hóa để trích xuất thông tin hữu ích từ dữ liệu. Bài lab này được thực hiện ở region Singapore (ap-southeast-1).\n"
},
{
	"uri": "http://localhost:1313/vi/6-analysis-visualize/6.1-athena/",
	"title": "Phân tích với Athena",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/vi/4-data-catalog/4.1-glue-crawler/",
	"title": "Tạo Glue Crawler",
	"tags": [],
	"description": "",
	"content": "Tạo Glue Crawler "
},
{
	"uri": "http://localhost:1313/vi/3-collection-storage/3.1-s3-bucket/",
	"title": "Tạo S3 bucket",
	"tags": [],
	"description": "",
	"content": "Tạo S3 bucket Tìm kiếm và chọn dịch vụ S3\nTrong giao diện S3, chọn Create bucket\nTại Bucket name, đặt tên asg-datalake-demo-2024\nKiểm tra các thông số như hình dưới:\nClick Create bucket\nTạo bucket thành công\n"
},
{
	"uri": "http://localhost:1313/vi/2-preparation/",
	"title": "Các bước chuẩn bị",
	"tags": [],
	"description": "",
	"content": "Các bước chuẩn bị Chúng ta sẽ chuẩn bị để tạo IAM role và gắn các policy cần thiết để đảm bảo AWS Glue có quyền truy cập vào dữ liệu trong Amazon S3 và thực hiện các công việc ETL một cách an toàn và hiệu quả.\nNội dung: Tạo IAM role Tạo policy "
},
{
	"uri": "http://localhost:1313/vi/4-data-catalog/4.2-test-data/",
	"title": "Kiểm tra dữ liệu",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/vi/3-collection-storage/3.2-object-in-bucket/",
	"title": "Tạo object cho S3 bucket",
	"tags": [],
	"description": "",
	"content": "Tạo object cho S3 bucket Tiếp theo, click vào bucket vừa tạo. Tại tab Object chọn Create folder\nTrong giao diện Create bucket, đặt Folder name là data\nKiểm tra các thông số và chọn Create folder\nTương tự với cách tạo folder, ta sẽ tạo folder reference_data ở trong folder data\nTạo folder reference_data thành công\nTiến hành download file tracks_list.json tại đây\nCtrl + S để lưu file Tiến hành upload file tracks_list.json lên folder reference_data\nChọn Object Chọn Upload Chọn Add file, chọn file đã tải Chọn file tracks_list.json Chọn Upload Upload file thành công "
},
{
	"uri": "http://localhost:1313/vi/2-preparation/2.2-policy/",
	"title": "Tạo policy",
	"tags": [],
	"description": "",
	"content": "Tạo policy Tiếp theo, ta sẽ tạo 1 policy.\nTại giao diện IAM\nChọn Policy Chọn Create policy Tại giao diện Create policy, ở bước Specify permission\nChọn JSON Paste đoạn code sau vào Policy Editor, chú ý để AccountID là account ID của bạn. Sau đó chọn Next {\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Action\u0026#34;: \u0026#34;iam:PassRole\u0026#34;,\r\u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:iam::AccountID:role/AWSGlueServiceRoleDefault\u0026#34;\r}\r]\r} Tại giao diện Create policy, ở bước Review and create\nĐặt Policy name là AWSGlueServicePolicy Review lại các thông số và chọn Create policy Tạo policy thành công "
},
{
	"uri": "http://localhost:1313/vi/6-analysis-visualize/6.2-quicksight/",
	"title": "Visualize với QuickSight",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/vi/2-preparation/2.3-attach-policy-to-role/",
	"title": "Gắn policy vào role",
	"tags": [],
	"description": "",
	"content": "Gắn policy vào role Ở phần này, ta gắn policy vào role đã tạo trước, để đảm bảo role có các quyền tương ứng được định nghĩa trong policy\nTrong IAM Dashboard\nChọn Role Chọn vào role AWSGlueServiceRoleDefault Trong chi tiết role AWSGlueServiceRoleDefault, chọn Attach policy\nChọn AWSGlueServicePolicy, click Add permission\nAttach policy vào role thành công\n"
},
{
	"uri": "http://localhost:1313/vi/3-collection-storage/3.3-kinesis-firehose-stream/",
	"title": "Tạo Firehose Stream",
	"tags": [],
	"description": "",
	"content": "Tạo Firehose Stream Tìm kiếm và chọn dịch vụ Kinesis\nChọn Amazon Data Firehose\nChọn Create Firehose Stream\nTại giao diện Create Firehose Stream\nTại Source chọn Direct PUT Tại Destination chọn Amazon S3 Tại Firehose stream name đặt tên FCJ-FirehoseStream Kiểm tra các giá trị ở Transform and convert records Ở Destination Setting\nChọn Browse Chọn bucket asg-datalake-demo-2024\nChọn Create\nKiểm tra Ở S3 bucket prefix, nhập data/raw\nỞ S3 bucket error output prefix, nhập data/error\nChọn Create Firehose stream\nTạo Firehose stream thành công "
},
{
	"uri": "http://localhost:1313/vi/3-collection-storage/",
	"title": "Thu thập và lưu trữ dữ liệu",
	"tags": [],
	"description": "",
	"content": "Trong hướng dẫn này, chúng ta sẽ tạo một S3 bucket để lưu trữ dữ liệu và tạo một Firehose stream để thu thập dữ liệu. Trong quá trình này, ta sẽ dùng ta sử dụng Kinesis Data Generator để tạo dữ liệu mẫu thông qua CloudFormation\nNội dung: Tạo S3 bucket Tạo Firehose Stream Tạo dữ liệu mẫu "
},
{
	"uri": "http://localhost:1313/vi/4-data-catalog/",
	"title": "Tạo Data Catalog",
	"tags": [],
	"description": "",
	"content": "Nội dung: Tạo Glue Crawler Kiểm tra dữ liệu "
},
{
	"uri": "http://localhost:1313/vi/3-collection-storage/3.4-sample-data/",
	"title": "Tạo dữ liệu mẫu",
	"tags": [],
	"description": "",
	"content": "Tạo dữ liệu mẫu Truy cập vào link, sẽ hiển thị giao diện sau: Click vào Help Chọn Create a Cognito User with CloudFormation Sau đó, sẽ dẫn đến giao diện Create Stack Lưu ý chuyển sang region Singapore (ap-southeast-1).\nKiểm tra các thông số, sau đó chọn Next Ở bước Specify stack details Stack name, nhập Kinesis-Data-Generator-Cognito-User Trong phần Parameters: Username: Nhập admin Password: Nhập password bạn muốn đặt Lưu ý theo như template của CloudFormation mà ta đang dùng, password phải có ít nhất 6 ký tự gồm chữ và số, trong đó chứa ít nhất một số\nỞ bước Configure stack options Để nguyên các giá trị mặc định Chọn I acknowledge that AWS CloudFormation might create IAM resources. Chọn Next Ở bước Review and create Kiểm tra lại stack\nSau đó chọn Submit\nĐợi quá trình Stack được tạo\nSau khi tạo thành công, kiểm tra phần Output Click vào đường link ở phần Output, ta được dẫn đến giao diện Amazon Kinesis Data Generator. Ta tiến hành đăng nhập với thông tin username và password đã tạo ở stack trước đó\nSetup các giá trị sau:\nRegion: ap-southeast-1 Stream/delivery stream: FCJ-FirehoseStream Records per second: 2000 Record template: Template 1 Nhập vào Template 1: {\r\u0026#34;uuid\u0026#34;: \u0026#34;{{random.uuid}}\u0026#34;,\r\u0026#34;device_ts\u0026#34;: \u0026#34;{{date.utc(\u0026#34;YYYY-MM-DD HH:mm:ss.SSS\u0026#34;)}}\u0026#34;,\r\u0026#34;device_id\u0026#34;: {{random.number(50)}},\r\u0026#34;device_temp\u0026#34;: {{random.weightedArrayElement(\r{\u0026#34;weights\u0026#34;:[0.30, 0.30, 0.20, 0.20],\u0026#34;data\u0026#34;:[32, 34, 28, 40]}\r)}},\r\u0026#34;track_id\u0026#34;: {{random.number(30)}},\r\u0026#34;activity_type\u0026#34;: {{random.weightedArrayElement(\r{\r\u0026#34;weights\u0026#34;: [0.1, 0.2, 0.2, 0.3, 0.2],\r\u0026#34;data\u0026#34;: [\u0026#34;\\\u0026#34;Running\\\u0026#34;\u0026#34;, \u0026#34;\\\u0026#34;Working\\\u0026#34;\u0026#34;, \u0026#34;\\\u0026#34;Walking\\\u0026#34;\u0026#34;, \u0026#34;\\\u0026#34;Traveling\\\u0026#34;\u0026#34;, \u0026#34;\\\u0026#34;Sitting\\\u0026#34;\u0026#34;]\r}\r)}}\r} Chọn Send Đợi quá trình generate cho tới khi nhận được khoảng 100.000 records thì chọn Stop Sending Data to Kinesis Tìm và chọn S3. Ta kiểm tra xem dữ liệu vừa generate đã đi tới S3 hay chưa. Kiểm tra dữ liệu đã đi tới S3 thành công "
},
{
	"uri": "http://localhost:1313/vi/5-data-transform/",
	"title": "Chuyển đổi dữ liệu",
	"tags": [],
	"description": "",
	"content": "Tạo SageMaker Notebook Bạn thực hiện tạo SageMaker Notebook bằng 2 cách (chọn 1 trong 2):\n"
},
{
	"uri": "http://localhost:1313/vi/6-analysis-visualize/",
	"title": "Phân tích và trực quan hóa",
	"tags": [],
	"description": "",
	"content": "Tổng quan về Amazon Athena Amazon Athena là một dịch vụ truy vấn tương tác được sử dụng để phân tích dữ liệu trong Amazon S3 bằng SQL tiêu chuẩn. Chúng ta chỉ cần trỏ đến dữ liệu của bạn trong Amazon S3, xác định schema và bắt đầu truy vấn bằng trình chỉnh sửa truy vấn tích hợp. Amazon Athena cho phép chúng ta khai thác tất cả dữ liệu của mình trong Amazon S3 mà không cần phải thiết lập các quy trình ETL phức tạp. Amazon Athena tính tiền dựa trên số lượng truy vấn được chạy.\nAmazon Athena sử dụng Presto với hỗ trợ SQL ANSI và làm việc với nhiều định dạng dữ liệu tiêu chuẩn, bao gồm CSV, JSON, ORC, Avro, và Parquet. Athena được khuyến nghị cho các nhu cầu truy vấn nhanh, nhưng cũng có khả năng xử lý phân tích phức tạp, bao gồm các phép join với lượng dữ liệu lớn, các window function và mảng.\nTổng quan về Amazon QuickSight Amazon QuickSight là một dịch vụ biểu diễn dữ liệu được quản lý hoàn toàn bởi AWS.\nData source là một kho lưu trữ dữ liệu bên ngoài và bạn cần cấu hình việc truy cập dữ liệu trong kho dữ liệu bên ngoài này, ví dụ Amazon S3, Amazon Athena, Salesforce, v.v.\nDataset xác định dữ liệu cụ thể trong Data source mà bạn muốn sử dụng. Ví dụ: Data source có thể là một bảng nếu bạn đang kết nối với cơ sở dữ liệu. Nó có thể là một file nếu bạn đang kết nối với Amazon S3.\nAnalysis là nơi chứa một tập hợp các Visual và câu chuyện liên quan, ví dụ như tất cả các câu chuyện áp dụng cho một mục tiêu kinh doanh nhất định hoặc KPI.\nVisual là một biểu diễn đồ họa của dữ liệu bạn. Bạn có thể tạo nhiều loại Visual khác nhau trong một Analysis, sử dụng các bộ dữ liệu và loại Visual khác nhau.\nDashboard là một trang bao gồm một hoặc nhiều Analysis chỉ cho phép xem, mà bạn có thể chia sẻ với người dùng Amazon QuickSight khác cho mục đích báo cáo. Dashboard giữ cấu hình của bản Analysis tại thời điểm bạn xuất bản nó, bao gồm các yếu tố như lọc, tham số, điều khiển và thứ tự sắp xếp.\nNội dung: Phân tích với Athena Visualize với QuickSight "
},
{
	"uri": "http://localhost:1313/vi/7-data-warehouse/",
	"title": "Data Warehouse",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/vi/8-clean-up/",
	"title": "Dọn dẹp tài nguyên",
	"tags": [],
	"description": "",
	"content": "Chúng ta sẽ dọn dẹp các tài nguyên sau:\n1. Xóa Visual QuickSight: Chọn Sheet cần xóa. Chọn Delete. 2. Xóa Analyses QuickSight: Chọn Analyses cần xóa. Chọn Delete. 3. Xóa Table database trong AWS Glue: Truy cập vào AWS Glue. Chọn Tables. Chọn các table cần xóa. Chọn Action. Chọn Delete để xác nhận xóa Table. 4. Xóa Database trong AWS Glue: Truy cập vào AWS Glue. Chọn Databases. Chọn database liên quan bài lab. Chọn Action. Chọn Delete xác nhận xóa database. 5. Xóa Notebook: Truy cập vào AWS Glue. Chọn Notebooks. Chọn aws-glue-notebook1. Chọn Action. Chọn Delete để xóa notebook. 6. Xóa development endpoints Truy cập AWS Glue. Chọn endpoint cần xóa. Chọn Action. Chọn Delete để xác nhận xóa endpoint. 7. Xóa AWS S3 bucket Truy cập vào AWS S3. Chọn Buckets. Chọn bucket liên quan bài lab. Chọn Empty. Nhập permanently delete, sau đó chọn Empty. Chọn lại bucket cần xóa, chọn Delete. Xác nhận xóa bucket. 8. Xóa stack CloudFormation Truy cập AWS CloudFormation. Chọn Stack. Chọn Stack liên quan bài lab. Chọn Delete. "
},
{
	"uri": "http://localhost:1313/vi/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/vi/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]